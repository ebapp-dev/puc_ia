{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b80cac7-20ff-47f1-a51a-65173da2dfc7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "#PUC-RIO\n",
    "#Aluno: Eduardo Baptistone\n",
    "\n",
    "##MVP - Engenharia de Dados\n",
    "\n",
    "###Objetivo\n",
    "\n",
    "A base de dados a ser analisada representa um sistema de controle de acesso de criança em eventos de instiruições religiosas. Nessa avaliação vamos analisar os dados da criança, do responsável e as ação de acesso no evento que fica registrado. As crianças podem ser classificadas pela idade ou pela sua condição, pois isso vai determinar o nível de cuidado que aquela criança exige. O objetivo central é observar a tendência de crescimento da presença de crianças e a proximidade geográfica que elas tem com a organização. Essas informações são variáveis no tempo, porque se tratanto de instituições religiosas, a presença podem ou não ocorrer naquele evento. As seguintes perguntas serão feitas a essa base de dados com o propósito de avaliar o objetivo principal:\n",
    "\n",
    "**Pergunta 1**<br>\n",
    "Qual a quantidade de crianças Especiais que frequentaram, em média por mês, para os eventos de domingo pela manhã, por organização, no primeiro semestre?<br><br>\n",
    "**Pergunta 2**<br>\n",
    "Qual a quantidade de crianças registradas por bairro na organização?<br><br>\n",
    "**Pergunta 3**<br>\n",
    "Qual média mensal de crianças do evento de domingo pela manhã, para classes kids, no primeiro semestre para a organização ibazs?<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "##Coleta de Dados\n",
    "\n",
    "Trabalho de MVP para a disciplina de Engenhraira de Dados onde devemos ser capaz de construir um pipeline de dados utilizando tecnologias na nuvem. O pipeline irá envolver a busca, coleta, modelagem, carga e análise dos dados.\n",
    "O dataset utilizado é baseado em um aplicação desenvolvida por mim, que controla acesso de crianças em eventos de instituição religiosa. O app se chama My Children.\n",
    "O banco principal do aplicativo é o Firebase. Como esse banco é NoSQL as informações da base são replicadas para o BigQuery. No BigQuery foram realizadas consultas para gerar os arquivos CSV utilzados no trabalho.\n",
    "Os dados sensíveis foram retirados ou anonimizados antes da coleta ser inserida no Databricks.\n",
    "\n",
    "![Coleta de dados](files/images/Coleta_bigquery.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d553036-c5e8-4442-804a-166336e1fbd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Os aquivos CSV são carregados manualmente no file system do Databricks para transformação em tabelas. A estrutura de tabelas segue um modelo de níveis de tratamento, onde cada nível representa um database de tabelas:\n",
    "LEVEL1 - tabelas com a estrutura e dados originais do CSV\n",
    "LEVEL2 - tabelas que passaram por processo de limpeza e inserção de novos campos\n",
    "LEVEL3 - tabelas dimensão, preparadas para alcançar o objetivo nas consultas\n",
    "\n",
    "\n",
    "| **Arquivos CSV** | **LEVEL 1**              | **LEVEL 2**                 | **LEVEL 3**               |\n",
    "|------------------|--------------------------|-----------------------------|---------------------------|\n",
    "| tb_children.csv  | tb_children<br>1937 rows | tb_children_l2<br>1923 rows | dim_children<br>1923 rows |\n",
    "| tb_events.csv    | tb_events<br>6239 rows   | tb_events_l2<br>6155 rows   | dim_events<br>6155 rows   |\n",
    "| tb_parents.csv   | tb_parents<br>486 rows   | tb_parents_l2<br>478 rows   | dim_parents<br>478 rows   |\n",
    "|                  |                          |                             | dim_classes<br>5 rows     |\n",
    "\n",
    "\n",
    "\n",
    "###Descrição das tabelas\n",
    "####tb_children\n",
    "Registro da criança no sistema com dados pessoais\n",
    "\n",
    "####tb_parents\n",
    "Registro dos pais ou responsáveis com dados pessoais\n",
    "\n",
    "####tb_events\n",
    "Registro da presença da criança no evento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa23c87d-1399-41d8-b5d3-be93f7a8f080",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Transformação dos arquivos CSV em Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- org: string (nullable = true)\n |-- mat: integer (nullable = true)\n |-- matResp: long (nullable = true)\n |-- codigo: integer (nullable = true)\n |-- dtnasc: date (nullable = true)\n |-- dtinicio: timestamp (nullable = true)\n |-- genero: string (nullable = true)\n |-- especial: boolean (nullable = true)\n |-- restriAlimentar: boolean (nullable = true)\n |-- visitante: boolean (nullable = true)\n\nThe df_children has 1937 rows.\n------------------------------\nroot\n |-- org: string (nullable = true)\n |-- mat: integer (nullable = true)\n |-- evento: timestamp (nullable = true)\n |-- dtEnt: date (nullable = true)\n |-- hrEnt: timestamp (nullable = true)\n |-- dtSai: date (nullable = true)\n |-- hrSai: timestamp (nullable = true)\n |-- dtCon: date (nullable = true)\n |-- hrCon: timestamp (nullable = true)\n\nThe df_events has 6239 rows.\n------------------------------\nroot\n |-- matResp: string (nullable = true)\n |-- org: string (nullable = true)\n |-- bairro: string (nullable = true)\n |-- cep: string (nullable = true)\n |-- cidade: string (nullable = true)\n\nThe df_parents has 486 rows.\n----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Transforma os arquivos CSV em dataframes organizando cabeçalhos e tipos dos dados\n",
    "# Localização dos arquivos CSV\n",
    "file_tb_children_location = \"/FileStore/tables/tb_children.csv\"\n",
    "file_tb_events_location = \"/FileStore/tables/tb_events.csv\"\n",
    "file_tb_parents_location = \"/FileStore/tables/tb_parents.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "delimiter = \",\"\n",
    "\n",
    "# TB_CHILDREN\n",
    "#  Aplicar options para formar o schema com os tipos corretos\n",
    "df_children = spark.read.format(file_type) \\\n",
    "  .option(\"treatEmptyValuesAsNulls\", \"true\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"mode\",\"DROPMALFORMED\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_tb_children_location)\n",
    "\n",
    "df_children.printSchema()\n",
    "\n",
    "# Count rows\n",
    "row_count = df_children.count()\n",
    "print(f'The df_children has {row_count} rows.')\n",
    "print(f'-'*30)\n",
    "\n",
    "# TB_EVENTS\n",
    "#  Aplicar options para formar o schema com os tipos corretos\n",
    "df_events = spark.read.format(file_type) \\\n",
    "  .option(\"treatEmptyValuesAsNulls\", \"true\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"mode\",\"DROPMALFORMED\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_tb_events_location)\n",
    "\n",
    "df_events.printSchema()\n",
    "\n",
    "# Count rows\n",
    "row_count = df_events.count()\n",
    "print(f'The df_events has {row_count} rows.')\n",
    "print(f'-'*30)\n",
    "\n",
    "# TB_PARENTS \n",
    "#  Aplicar options para formar o schema com os tipos corretos\n",
    "df_parents = spark.read.format(file_type) \\\n",
    "  .option(\"treatEmptyValuesAsNulls\", \"true\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"mode\",\"DROPMALFORMED\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_tb_parents_location)\n",
    "\n",
    "df_parents.printSchema()\n",
    "\n",
    "# Count rows\n",
    "row_count = df_parents.count()\n",
    "print(f'The df_parents has {row_count} rows.')\n",
    "print(f'-'*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ecbc2a-c13c-48f7-98a0-2ee5ea5e3eed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Transforma os dataframes em tabelas e armazena no file system do Databricks\n",
    "\n",
    "#Apaga todas as tabelas do file system\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/level1.db/tb_children', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/level1.db/tb_events', True)\n",
    "dbutils.fs.rm('dbfs:/user/hive/warehouse/level1.db/tb_parents', True)\n",
    "\n",
    "\n",
    "# Cria database LEVEL1 para receber as tabelas\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS level1\")\n",
    "\n",
    "# Cria tabelas no database default\n",
    "\n",
    "# TB_CHILDREN\n",
    "\n",
    "permanent_table_name = \"level1.tb_children\"\n",
    "query = f\"DROP TABLE IF EXISTS {permanent_table_name}\"\n",
    "spark.sql(query)\n",
    "#spark.sql(f\"CREATE TABLE {permanent_table_name}\")\n",
    "\n",
    "df_children.write.format(\"delta\").options(mode='overwrite').saveAsTable(permanent_table_name)\n",
    "\n",
    "# TB_EVENTS\n",
    "permanent_table_name = \"level1.tb_events\"\n",
    "query = f\"DROP TABLE IF EXISTS {permanent_table_name}\"\n",
    "spark.sql(query)\n",
    "\n",
    "df_events.write.format(\"delta\").options(mode='overwrite').saveAsTable(permanent_table_name)\n",
    "\n",
    "# TB_PARENTS\n",
    "permanent_table_name = \"level1.tb_parents\"\n",
    "query = f\"DROP TABLE IF EXISTS {permanent_table_name}\"\n",
    "spark.sql(query)\n",
    "\n",
    "df_parents.write.format(\"delta\").options(mode='overwrite').saveAsTable(permanent_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b1a8ea8-1313-4389-b12b-862176d44285",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run \"./Tabelas LEVEL 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99adb343-e104-44a8-aaa8-e428668479af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run \"./Tabelas LEVEL 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c4f4de-3f74-4171-949f-8d6d9a336982",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run \"./Tabelas LEVEL 3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e476cc86-7a37-4e69-9506-c61c259bd202",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run \"./Pergunta 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceba730a-ff36-45fe-8e27-d79e87f43c62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run \"./Pergunta 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca776bd-ee7d-43c0-bb2b-3497eaeb4662",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run \"./Pergunta 3 - Análise Final\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Início - Objetivo e Coleta",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
